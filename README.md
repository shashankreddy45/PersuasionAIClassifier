# PersuasionAIClassifier


his project aims to develop an AI-powered tool for analyzing persuasion arguments. It leverages machine learning to:
Classify whether an argument is human-written or generated by an AI model.

Assess the quality of arguments based on linguistic features such as vocabulary diversity, sentence complexity, and part-of-speech ratios.

Provide detailed feedback to improve argument quality.

The dataset used for this project is the "Anthropic/persuasion" dataset from Hugging Face Datasets, which contains claims and arguments along with their sources (human or AI) and persuasiveness ratings. This makes it ideal for training models to distinguish between human and AI-generated content while also evaluating argument quality.
Features
Source Classification: Accurately classifies arguments as human-written or AI-generated.

Quality Assessment: Evaluates argument quality using advanced linguistic features.

Detailed Feedback: Provides actionable suggestions for improving argument quality.

Visualizations: Includes plots and charts to visualize key metrics and analysis results.

Extensibility: Designed to be extended for other datasets or additional features like sentiment analysis or coherence scoring.

Dataset
The project uses the "Anthropic/persuasion" dataset from Hugging Face Datasets. This dataset contains:
worker_id: ID of the participant who annotated their initial and final stance on the claim.

claim: The claim for which the argument was generated.

argument: The generated argument (human or AI).

source: The source of the argument (e.g., "Human", "Claude 2", etc.).

prompt_type: The type of prompt used to generate the argument (e.g., "Expert Writer Rhetorics", "Compelling Case").

rating_initial: The participant’s initial rating of the claim (on a scale from 1 to 7).

rating_final: The participant’s final rating of the claim after reading the argument (on the same scale).

Additionally:
persuasiveness_metric: A derived metric calculated as rating_final - rating_initial, indicating how persuasive the argument was in changing the participant’s stance (computed in the code).

Dataset Link
Technologies Used
Python: For scripting and data manipulation.

Pandas: For data handling and analysis.

NumPy: For numerical operations.

Scikit-learn: For machine learning utilities and metrics.

TensorFlow: For building and training neural network models.

SpaCy: For natural language processing tasks like tokenization and part-of-speech tagging.

TextBlob: For sentiment analysis and additional text features.

Matplotlib and Seaborn: For data visualization.

Jupyter Notebook: For interactive development and analysis.

Model Architecture and Training
The classification model, based on the provided code, involves:
Feature extraction using TF-IDF on combined claim and argument text, resulting in a matrix of shape (3939, 2000).

Model training using a logistic regression classifier, as seen in the code, with potential for neural network models using TensorFlow for improved performance.

Evaluation metrics include precision, recall, and F1-score, with the code showing a classification report for the test set.

The model achieves varying performance across classes, with some classes (e.g., -1, 2, 3, 4) having low precision due to imbalance, as indicated by the warnings for undefined metrics.
Feature Extraction and Analysis Functions
The project includes comprehensive feature extraction, covering:
Text Preprocessing: Combining claim and argument for TF-IDF vectorization, removing stop words, and potentially using SpaCy for advanced NLP features.

Basic Statistics: Word count, sentence count, and other text statistics from the argument text.

Linguistic Features: Part-of-speech ratios, sentiment scores using TextBlob, and potentially coherence metrics.

Persuasiveness Analysis: Derived from the difference between rating_initial and rating_final, providing a metric for argument effectiveness.

Functions like analyze_single_argument (hypothetical, based on similar projects) would allow for individual argument analysis, providing detailed reports and visualizations.
Results and Performance
The model evaluation, as shown in the code, provides a classification report with metrics:
Accuracy: 0.62

Precision, recall, and F1-score vary by class, with class 0 (persuasiveness_metric = 0) having the highest performance (precision 0.63, recall 0.97, F1-score 0.76).

An unexpected detail is the model's struggle with imbalanced classes, leading to undefined metrics for some labels, which might surprise users expecting uniform performance across all persuasiveness levels.
Future Work and Extensions
Potential future enhancements include:
Implementing neural network models for better classification performance, especially for imbalanced classes.

Developing a web application for user-friendly argument analysis, making it accessible to researchers and educators.

Expanding the feature set with additional NLP techniques, such as coherence analysis or readability scores.

Exploring unsupervised methods for argument quality assessment, potentially identifying patterns without labeled data.

